# Model Configuration for LLM API
# This file controls which model is loaded and how it behaves

# Model Selection
model:
  name: "microsoft/phi-3-mini-4k-instruct"  # Phi-3 Mini 4K Instruct
  type: "causal_lm"
  max_length: 512
  temperature: 0.7

# Alternative Models (uncomment to use)
# model:
#   name: "microsoft/DialoGPT-medium"  # Medium conversational model
#   type: "causal_lm"
#   max_length: 512
#   temperature: 0.7

# model:
#   name: "gpt2"  # Smaller, faster model
#   type: "causal_lm"
#   max_length: 256
#   temperature: 0.7

# model:
#   name: "microsoft/DialoGPT-large"  # Larger, more accurate model
#   type: "causal_lm"
#   max_length: 1024
#   temperature: 0.7

# Quantization (for memory efficiency)
quantization:
  enabled: true   # set to true for Phi-3 
  method: "4bit"  # use 4-bit quantization for efficiency

# Performance Settings
performance:
  batch_size: 1
  max_concurrent_requests: 10
