# Model Configuration for LLM API
# This file controls which model is loaded and how it behaves

# Model Selection
model:
  name: "microsoft/DialoGPT-medium"  # Change this to use different models
  type: "causal_lm"
  max_length: 512
  temperature: 0.7

# Alternative Models (uncomment to use)
# model:
#   name: "gpt2"  # Smaller, faster model
#   type: "causal_lm"
#   max_length: 256
#   temperature: 0.7

# model:
#   name: "microsoft/DialoGPT-large"  # Larger, more accurate model
#   type: "causal_lm"
#   max_length: 1024
#   temperature: 0.7

# Quantization (for memory efficiency)
quantization:
  enabled: false  # Set to true to enable 4-bit quantization
  method: "4bit"

# Performance Settings
performance:
  batch_size: 1
  max_concurrent_requests: 10